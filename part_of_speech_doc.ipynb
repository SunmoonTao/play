{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.shared import Pt\n",
    "from docx.enum.dml import MSO_THEME_COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_with_punc(word):\n",
    "    punc_r = [\".\",\";\",\":\",\"!\",\"?\",\"/\",\"\\\\\",\",\",\"#\",\"@\",\"$\",\"&\",\"\\\"\",\"-\"]\n",
    "\n",
    "    if word[-1] in punc_r:\n",
    "        word=word[-1]+word[:-1]\n",
    "    elif ')' in word:\n",
    "        word='(' + word[:-1]\n",
    "    elif ']' in word:\n",
    "        word='[' + word[:-1]\n",
    "    elif '}' in word:\n",
    "        word='[' + word[:-1]\n",
    "    elif '(' in word:\n",
    "        word=word[1:]+')'\n",
    "    elif '[' in word:\n",
    "        word= word[1:]+']'\n",
    "    elif '{' in word:\n",
    "        word= word[1:]+'}'\n",
    "    else:\n",
    "        word=word\n",
    "    return (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sentense(row):\n",
    "    output=list()\n",
    "    row.reverse()\n",
    "    for index,w in enumerate(row):\n",
    "        output.append(word_with_punc(w))\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('outflow1.txt','r')\n",
    "text=f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "document = Document()\n",
    "\n",
    "document.add_heading('Document Title', 0)\n",
    "\n",
    "p = document.add_paragraph('A plain paragraph having some ')\n",
    "p.add_run('bold').bold = True\n",
    "p.add_run(' and some ')\n",
    "p.add_run('italic.').italic = True\n",
    "\n",
    "document.add_heading('Heading, level 1', level=1)\n",
    "document.add_paragraph('Intense quote', style='Intense Quote')\n",
    "\n",
    "paragraph = document.add_paragraph('Normal text, ')\n",
    "run = paragraph.add_run(text)\n",
    "run.style = 'Emphasis'\n",
    "font = run.font\n",
    "font.name = 'Calibri'\n",
    "font.size = Pt(12)\n",
    "from docx.shared import RGBColor\n",
    "font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "\n",
    "document.add_paragraph(\n",
    "    'first item in unordered list', style='List Bullet'\n",
    ")\n",
    "document.add_paragraph(\n",
    "    'first item in ordered list', style='List Number'\n",
    ")\n",
    "\n",
    "# document.add_picture('monty-truth.png', width=Inches(1.25))\n",
    "\n",
    "records = (\n",
    "    (3, '101', 'Spam'),\n",
    "    (7, '422', 'Eggs'),\n",
    "    (4, '631', 'Spam, spam, eggs, and spam')\n",
    ")\n",
    "\n",
    "table = document.add_table(rows=1, cols=3)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Qty'\n",
    "hdr_cells[1].text = 'Id'\n",
    "hdr_cells[2].text = 'Desc'\n",
    "for qty, id, desc in records:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = str(qty)\n",
    "    row_cells[1].text = id\n",
    "    row_cells[2].text = desc\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Python for NLP: Vocabulary and Phrase Matching with SpaCy \n",
      " \n",
      ">>> By â€¢ \n",
      " \n",
      ">>> This is the third article in this series of articles on Python for \n",
      " NLTK Python's how saw we , the In .Processing Language Natural <<< \n",
      " >>> and libraries can be used to perform simple NLP tasks such as , . We \n",
      " entity named ,tagging speech of parts perform to how saw also <<< \n",
      " >>> recognition and noun-parsing. However, all of these operations are \n",
      "                                               .words individual on performed <<< \n",
      " \n",
      ">>> In this article, we will move a step further and explore vocabulary \n",
      " patterns define will We .library spaCy the using matching phrase and <<< \n",
      " >>> and then will see which phrases that match the pattern we define. This \n",
      " of parts involve that expressions regular defining to similar is <<< \n",
      " >>> speech. \n",
      " \n",
      ">>> Rule-Based Matching \n",
      " \n",
      ">>> The spaCy library comes with Matcher tool that can be used to specify \n",
      " tool Matcher the use to process The .matching phrase for rules custom <<< \n",
      " >>> is pretty straight forward. The first thing you have to do is define \n",
      " the add to have you ,Next .match to want you that patterns the <<< \n",
      " >>> patterns to the Matcher tool and finally, you have to apply \n",
      " rules your match to want you that document the to tool Matcher the <<< \n",
      " >>> with. This is best explained with the help of an example. \n",
      " \n",
      ">>> For rule-based matching, you need to perform the following steps: \n",
      " \n",
      ">>> Creating Matcher Object \n",
      " \n",
      ">>> The first step is to create the matcher object: \n",
      " \n",
      ">>> import spacy \n",
      " \n",
      ">>> nlp = spacy.load('en_core_web_sm') \n",
      " \n",
      "\n",
      ">>> from spacy.matcher import Matcher \n",
      " \n",
      ">>> m_tool = Matcher(nlp.vocab) \n",
      " \n",
      ">>> Defining Patterns \n",
      " \n",
      ">>> The next step is to define the patterns that will be used to filter \n",
      " -\"quick-brown phrases the find to want we Suppose .phrases similar <<< \n",
      " >>> fox\", \"quick brown fox\", \"quickbrownfox\" or \"quick brownfox\". To do \n",
      " :patterns four following the create to need we ,so <<< \n",
      " \n",
      ">>> p1 = [{'LOWER': 'quickbrownfox'}] \n",
      " \n",
      ">>> p2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, \n",
      " ['fox'} :{'LOWER' ,True} :{'IS_PUNCT' <<< \n",
      " \n",
      ">>> p3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}] \n",
      " \n",
      ">>> p4 = [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}] \n",
      " \n",
      ">>> In the above script, \n",
      " \n",
      ">>> p1 looks for the phrase \"quickbrownfox\" \n",
      " \n",
      ">>> p2 looks for the phrase \"quick-brown-fox\" \n",
      " \n",
      ">>> p3 tries to search for \"qucik brown fox\" \n",
      " \n",
      ">>> p4 looks for the phrase \"quick brownfox\" \n",
      " \n",
      ">>> The token attribute LOWER defines that the phrase should be converted \n",
      "                                               .matching before case lower into <<< \n",
      " \n",
      ">>> Once the patterns are defined, we need to add them to \n",
      " .earlier created we that object Matcher the <<< \n",
      " \n",
      ">>> m_tool.add('QBF', None, p1, p2, p3, p4) \n",
      " \n",
      ">>> Here \"QBF\" is the name of our matcher. You can give it any name. \n",
      " \n",
      ">>> Applying Matcher to the Document \n",
      " \n",
      ">>> We have our matcher ready. The next step is to apply the matcher on a \n",
      " a create first Let's .match any get can we if see and document text <<< \n",
      " >>> simple document: \n",
      " \n",
      ">>> sentence = nlp(u'The quick-brown-fox jumps over the lazy dog. The \n",
      "                                               \\ .well eats fox brown quick <<< \n",
      " \n",
      ">>> the quickbrownfox is dead. the dog misses the quick \n",
      "                                               (brownfox' <<< \n",
      " \n",
      ">>> To apply the matcher to a document. The document is needed to be \n",
      " all be will result The .object matcher the to parameter a as passed <<< \n",
      " >>> the ids of the phrases matched in the document, along with their \n",
      " following the Execute .document the in positions ending and starting <<< \n",
      " >>> script: \n",
      " \n",
      ">>> phrase_matches = m_tool(sentence) \n",
      " \n",
      ">>> print(phrase_matches ) \n",
      " \n",
      ">>> The output of the script above looks like this: \n",
      " \n",
      ">>> [(12825528024649263697, 1, 6), (12825528024649263697, 13, 16), \n",
      " (31) ,29 ,(12825528024649263697 ,22) ,21 ,(12825528024649263697 <<< \n",
      " \n",
      ">>> From the output, you can see that four phrases have been matched. The \n",
      " the ,matched phrase the of id the is output each in number long first <<< \n",
      " >>> second and third numbers are the starting and ending positions of the \n",
      "                                               .phrase <<< \n",
      " \n",
      ">>> To actually view the result in a better way, we can iterate through \n",
      " the Execute .value string its display and phrase matched each <<< \n",
      " >>> following script: \n",
      " \n",
      ">>> for match_id, start, end in phrase_matches: \n",
      " \n",
      ">>> string_id = nlp.vocab.strings[match_id] \n",
      " \n",
      ">>> span = sentence[start:end] \n",
      " \n",
      ">>> print(match_id, string_id, start, end, span.text) \n",
      " \n",
      ">>> Output: \n",
      " \n",
      ">>> 12825528024649263697 QBF 1 6 quick-brown-fox \n",
      " \n",
      ">>> 12825528024649263697 QBF 13 16 quick brown fox \n",
      " \n",
      ">>> 12825528024649263697 QBF 21 22 quickbrownfox \n",
      " \n",
      ">>> 12825528024649263697 QBF 29 31 quick brownfox \n",
      " \n",
      ">>> From the output, you can see all the matched phrases along with their \n",
      " .position ending and start and ids vocabulary <<< \n",
      " \n",
      ">>> More Options for Rule-Based Matching \n",
      " \n",
      ">>> Official documentation from the sPacy library contains details of all \n",
      " .matching phrase for used be can that the <<< \n",
      " \n",
      ">>> For instance, the \"*\" attribute is defined to search for one or more \n",
      "                                               .token the of instances <<< \n",
      " \n",
      ">>> Let's write a simple pattern that can identify the phrase \"quick-- \n",
      "                                               .quick-brown---fox or \"brown--fox <<< \n",
      " \n",
      ">>> Let's first remove the previous matcher QBF. \n",
      " \n",
      ">>> m_tool.remove('QBF') \n",
      " \n",
      ">>> Next, we need to define our new pattern: \n",
      " \n",
      "\n",
      ">>> p1 = [{'LOWER': 'quick'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': \n",
      " ['fox'} :{'LOWER' ,'OP':'*'} ,True :{'IS_PUNCT' ,'brown'} <<< \n",
      " \n",
      ">>> m_tool.add('QBF', None, p1) \n",
      " \n",
      ">>> The pattern p1 will match all the phrases where there are one or more \n",
      " our define now Let's .fox brown quick phrase the in punctuations <<< \n",
      " >>> document for filtering: \n",
      " \n",
      ">>> sentence = nlp(u'The quick--brown--fox jumps over the quick-brown--- \n",
      "                                               (fox' <<< \n",
      " \n",
      ">>> You can see our document has two phrases quick--brown--fox and quick- \n",
      " mather our apply Let's .pattern our match should you that ,brown---fox <<< \n",
      " >>> to the document and see the results: \n",
      " \n",
      ">>> phrase_matches = m_tool(sentence) \n",
      " \n",
      "\n",
      ">>> for match_id, start, end in phrase_matches: \n",
      " \n",
      ">>> string_id = nlp.vocab.strings[match_id] \n",
      " \n",
      ">>> span = sentence[start:end] \n",
      " \n",
      ">>> print(match_id, string_id, start, end, span.text) \n",
      " \n",
      ">>> The output of the script above looks like this: \n",
      " \n",
      ">>> 12825528024649263697 QBF 1 6 quick--brown--fox \n",
      " \n",
      ">>> 12825528024649263697 QBF 10 15 quick-brown---fox \n",
      " \n",
      ">>> From the output, you can see that our matcher has successfully matched \n",
      "                                               .phrases two the <<< \n",
      " \n",
      ">>> Subscribe to our Newsletter \n",
      " \n",
      ">>> Top of Form \n",
      " \n",
      ">>> Get occassional tutorials, guides, and jobs in your inbox. No spam \n",
      "                                               .time any at Unsubscribe .ever <<< \n",
      " \n",
      ">>> Newsletter Signup \n",
      " \n",
      ">>> Subscribe \n",
      " \n",
      ">>> Bottom of Form \n",
      " \n",
      ">>> Phrase-Based Matching \n",
      " \n",
      ">>> In the last section, we saw how we can define rules that can be used \n",
      " ,rules defining to addition In .document the from phrases identify to <<< \n",
      " >>> we can directly specify the phrases that we are looking for. This is a \n",
      " .matching phrase of way efficient more <<< \n",
      " \n",
      ">>> In this section, we will be doing phrase matching inside a Wikipedia \n",
      " .intelligence Artificial on article <<< \n",
      " \n",
      ">>> Before we see the steps to perform phrase-matching, let's first parse \n",
      " phrase perform to using be will we that article Wikipedia the <<< \n",
      " >>> matching. Execute the following script: \n",
      " \n",
      ">>> import bs4 as bs \n",
      " \n",
      ">>> import urllib.request \n",
      " \n",
      ">>> import re \n",
      " \n",
      ">>> import nltk \n",
      " \n",
      "\n",
      ">>> scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/ \n",
      "                                               (Artificial_intelligence' <<< \n",
      " \n",
      ">>> article = scrapped_data .read() \n",
      " \n",
      "\n",
      ">>> parsed_article = bs.BeautifulSoup(article,'lxml') \n",
      " \n",
      "\n",
      ">>> paragraphs = parsed_article.find_all('p') \n",
      " \n",
      "\n",
      ">>> article_text = \"\" \n",
      " \n",
      "\n",
      ">>> for p in paragraphs: \n",
      " \n",
      ">>> article_text += p.text \n",
      " \n",
      "\n",
      "\n",
      ">>> processed_article = article_text.lower() \n",
      " \n",
      ">>> processed_article = re.sub('[^a-zA-Z]', ' ', processed_article ) \n",
      " \n",
      ">>> processed_article = re.sub(r'\\s+', ' ', processed_article) \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> The script has been explained in detail in my article on You can go \n",
      " in works parsing how understand to want you if article the read and <<< \n",
      " >>> Python. \n",
      " \n",
      ">>> The processed_article contains the document that we will use for \n",
      "                                               .phrase-matching <<< \n",
      " \n",
      ">>> The steps to perform phrase matching are quite similar to rule based \n",
      "                                               .matching <<< \n",
      " \n",
      ">>> Create Phrase Matcher Object \n",
      " \n",
      ">>> As a first step, you need to create PhraseMatcher object. The \n",
      "                                               :that does script following <<< \n",
      " \n",
      ">>> import spacy \n",
      " \n",
      ">>> nlp = spacy.load('en_core_web_sm') \n",
      " \n",
      "\n",
      "\n",
      ">>> from spacy.matcher import PhraseMatcher \n",
      " \n",
      ">>> phrase_matcher = PhraseMatcher(nlp.vocab) \n",
      " \n",
      ">>> Notice in the previous section we created Matcher object. Here, in \n",
      " .object PhraseMathcer creating are we ,case this <<< \n",
      " \n",
      ">>> Create Phrase List \n",
      " \n",
      ">>> In the second step, you need to create a list of phrases to match and \n",
      " following the in shown as documents NLP spaCy to list the convert then <<< \n",
      " >>> script: \n",
      " \n",
      ">>> phrases = ['machine learning', 'robots', 'intelligent agents'] \n",
      " \n",
      "\n",
      ">>> patterns = [nlp(text) for text in phrases] \n",
      " \n",
      ">>> Finally, you need to add your phrase list to the phrase matcher. \n",
      " \n",
      ">>> phrase_matcher.add('AI', None, *patterns) \n",
      " \n",
      ">>> Here the name of our matcher is AI. \n",
      " \n",
      ">>> Applying Matcher to the Document \n",
      " \n",
      ">>> Like rule-based matching, we again need to apply our phrase matcher to \n",
      " document spaCy in not is article parsed our ,However .document the <<< \n",
      " >>> format. Therefore, we will convert our article into sPacy document \n",
      " .article the to matcher phrase our apply then will and format <<< \n",
      " \n",
      ">>> sentence = nlp (processed_article) \n",
      " \n",
      "\n",
      ">>> matched_phrases = phrase_matcher(sentence) \n",
      " \n",
      ">>> In the output, we will have all the ids of all the matched phrases \n",
      " :below shown as document the in indexes end and start their with along <<< \n",
      " \n",
      ">>> [(5530044837203964789, 37, 39), \n",
      " \n",
      ">>> (5530044837203964789, 402, 404), \n",
      " \n",
      ">>> (5530044837203964789, 693, 694), \n",
      " \n",
      ">>> (5530044837203964789, 1284, 1286), \n",
      " \n",
      ">>> (5530044837203964789, 3059, 3061), \n",
      " \n",
      ">>> (5530044837203964789, 3218, 3220), \n",
      " \n",
      ">>> (5530044837203964789, 3753, 3754), \n",
      " \n",
      ">>> (5530044837203964789, 5212, 5213), \n",
      " \n",
      ">>> (5530044837203964789, 5287, 5288), \n",
      " \n",
      ">>> (5530044837203964789, 6769, 6771), \n",
      " \n",
      ">>> (5530044837203964789, 6781, 6783), \n",
      " \n",
      ">>> (5530044837203964789, 7496, 7498), \n",
      " \n",
      ">>> (5530044837203964789, 7635, 7637), \n",
      " \n",
      ">>> (5530044837203964789, 8002, 8004), \n",
      " \n",
      ">>> (5530044837203964789, 9461, 9462), \n",
      " \n",
      ">>> (5530044837203964789, 9955, 9957), \n",
      " \n",
      ">>> (5530044837203964789, 10784, 10785), \n",
      " \n",
      ">>> (5530044837203964789, 11250, 11251), \n",
      " \n",
      ">>> (5530044837203964789, 12290, 12291), \n",
      " \n",
      ">>> (5530044837203964789, 12411, 12412), \n",
      " \n",
      ">>> (5530044837203964789, 12455, 12456)] \n",
      " \n",
      ">>> To see the string value of the matched phrases, execute the following \n",
      "                                               :script <<< \n",
      " \n",
      ">>> for match_id, start, end in matched_phrases: \n",
      " \n",
      ">>> string_id = nlp.vocab.strings[match_id] \n",
      " \n",
      ">>> span = sentence[start:end] \n",
      " \n",
      ">>> print(match_id, string_id, start, end, span.text) \n",
      " \n",
      ">>> In the output, you will see the strig value of the matched phrases as \n",
      "                                               :below shown <<< \n",
      " \n",
      ">>> 5530044837203964789 AI 37 39 intelligent agents \n",
      " \n",
      ">>> 5530044837203964789 AI 402 404 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 693 694 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 1284 1286 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 3059 3061 intelligent agents \n",
      " \n",
      ">>> 5530044837203964789 AI 3218 3220 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 3753 3754 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 5212 5213 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 5287 5288 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 6769 6771 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 6781 6783 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 7496 7498 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 7635 7637 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 8002 8004 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 9461 9462 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 9955 9957 machine learning \n",
      " \n",
      ">>> 5530044837203964789 AI 10784 10785 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 11250 11251 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 12290 12291 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 12411 12412 robots \n",
      " \n",
      ">>> 5530044837203964789 AI 12455 12456 robots \n",
      " \n",
      ">>> From the output, you can see all the three phrases that we tried to \n",
      " .ids string the and index end and start their with along search <<< \n",
      " \n",
      ">>> Stop Words \n",
      " \n",
      ">>> Before we conclude this article, I just wanted to touch the concept of \n",
      " etc \"\"an ,\"a\" ,\"the\" as such words English are words Stop .words stop <<< \n",
      " >>> that do not have any meaning of their own. Stop words are often not \n",
      " language or classification text as such tasks NLP for useful very <<< \n",
      " >>> modeling. So it is often better to remove these stop words before \n",
      " .document the of processing further <<< \n",
      " \n",
      ">>> The spaCy library contains 305 stop words. In addition, depending upon \n",
      " spaCy the from words stop remove or add also can we ,requirements our <<< \n",
      " >>> library. \n",
      " \n",
      ">>> To see the default spaCy stop words, we can use stop_words attribute \n",
      "                                               :below shown as model spaCy the of <<< \n",
      " \n",
      ">>> import spacy \n",
      " \n",
      ">>> sp = spacy.load('en_core_web_sm') \n",
      " \n",
      ">>> print(sp.Defaults.stop_words) \n",
      " \n",
      ">>> In the output, you will see all the sPacy stop words: \n",
      " \n",
      ">>> {'less', 'except', 'top', 'me', 'three', 'fifteen', 'a', 'is', \n",
      " ,'any' ,'has' ,'must' ,'without' ,'everyone' ,'then' ,'all' ,'those' <<< \n",
      " >>> 'anyhow', 'keep', 'through', 'bottom', 'get', 'indeed', 'it', 'still', \n",
      " ,'myself' ,'various' ,'eight' ,'though' ,'doing' ,'whatever' ,'ten' <<< \n",
      " >>> 'across', 'wherever', 'himself', 'always', 'thus', 'am', 'after', \n",
      " ,'regarding' ,'rather' ,'own' ,'down' ,'at' ,'perhaps' ,'should' <<< \n",
      " >>> 'which', 'anywhere', 'whence', 'would', 'been', 'how', 'herself', \n",
      " ,'from' ,'alone' ,'seems' ,'every' ,'behind' ,'please' ,'might' ,'now' <<< \n",
      " >>> 'via', 'its', 'become', 'hers', 'there', 'front', 'whose', 'before', \n",
      " ,'eleven' ,'five' ,'two' ,'whither' ,'up' ,'whereafter' ,'against' <<< \n",
      " >>> 'why', 'below', 'out', 'whereas', 'serious', 'six', 'give', 'also', \n",
      " ,'have' ,'else' ,'onto' ,'again' ,'none' ,'anyway' ,'his' ,'became' <<< \n",
      " >>> 'few', 'thereby', 'whoever', 'yet', 'part', 'just', 'afterwards', \n",
      " ,'therefore' ,'once' ,'can' ,'not' ,'hereby' ,'see' ,'mostly' <<< \n",
      " >>> 'together', 'whom', 'elsewhere', 'beforehand', 'themselves', 'with', \n",
      " ,'becoming' ,'who' ,'are' ,'former' ,'upon' ,'many' ,'seem' <<< \n",
      " >>> 'formerly', 'between', 'cannot', 'him', 'that', 'first', 'more', \n",
      " ,'whereupon' ,'my' ,'whereby' ,'under' ,'whenever' ,'although' <<< \n",
      " >>> 'anyone', 'toward', 'by', 'four', 'since', 'amongst', 'move', 'each', \n",
      " ,'when' ,'name' ,'if' ,'used' ,'besides' ,'as' ,'somehow' ,'forty' <<< \n",
      " >>> 'ever', 'however', 'otherwise', 'hundred', 'moreover', 'your', \n",
      " ,'enough' ,'her' ,'where' ,'another' ,'empty' ,'the' ,'sometimes' <<< \n",
      " >>> 'quite', 'throughout', 'anything', 'she', 'and', 'does', 'above', \n",
      " ,'re' ,'off' ,'nobody' ,'made' ,'back' ,'this' ,'in' ,'show' ,'within' <<< \n",
      " >>> 'meanwhile', 'than', 'neither', 'twenty', 'call', 'you', 'next', \n",
      " ,'latterly' ,'such' ,'seemed' ,'or' ,'go' ,'therein' ,'thereupon' <<< \n",
      " >>> 'already', 'mine', 'yourself', 'an', 'amount', 'hereupon', 'namely', \n",
      " ,'whole' ,'done' ,'be' ,'could' ,'yours' ,'of' ,'their' ,'same' <<< \n",
      " >>> 'seeming', 'someone', 'these', 'towards', 'among', 'becomes', 'per', \n",
      " ,'make' ,'well' ,'ours' ,'latter' ,'both' ,'beside' ,'beyond' ,'thru' <<< \n",
      " >>> 'nowhere', 'about', 'were', 'others', 'due', 'yourselves', 'unless', \n",
      " ,'something' ,'our' ,'everything' ,'most' ,'too' ,'even' ,'thereafter' <<< \n",
      " >>> 'did', 'using', 'full', 'while', 'will', 'only', 'nor', 'often', \n",
      " ,'very' ,'was' ,'along' ,'some' ,'over' ,'least' ,'being' ,'side' <<< \n",
      " >>> 'on', 'into', 'nine', 'noone', 'several', 'i', 'one', 'third', \n",
      " ,'either' ,'because' ,'whether' ,'here' ,'further' ,'but' ,'herein' <<< \n",
      " >>> 'hereafter', 'really', 'so', 'somewhere', 'we', 'nevertheless', \n",
      " ,'everywhere' ,'ca' ,'almost' ,'thence' ,'they' ,'had' ,'last' <<< \n",
      " >>> 'itself', 'no', 'ourselves', 'may', 'wherein', 'take', 'around', \n",
      " ,'twelve' ,'say' ,'what' ,'do' ,'until' ,'to' ,'them' ,'never' <<< \n",
      " >>> 'nothing', 'during', 'sixty', 'sometime', 'us', 'fifty', 'much', \n",
      " ['put' ,'he' ,'hence' ,'other' ,'for' <<< \n",
      " \n",
      ">>> You can also check if a word is a stop word or not. To do so, you can \n",
      " :below shown as attribute is_stop the use <<< \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> sp.vocab['wonder'].is_stop \n",
      " \n",
      ">>> Since \"wonder\" is not a spaCy stop word, you will see False in the \n",
      "                                               .output <<< \n",
      " \n",
      ">>> To add or remove stopwords in spaCy, you can use sp.Defaults.stop_word \n",
      " .respectively methods (sp.Defaults.stop_words.remove( and (s.add( <<< \n",
      " \n",
      ">>> sp.Defaults.stop_words.add('wonder') \n",
      " \n",
      ">>> Next, we need to set the is_stop tag for wonder to 'True` as shown \n",
      "                                               :below <<< \n",
      " \n",
      ">>> sp.vocab['wonder'].is_stop = True \n",
      " \n",
      ">>> Conclusion \n",
      " \n",
      ">>> Phrase and vocabulary matching is one of the most important natural \n",
      " our continued we ,article this In .tasks processing language <<< \n",
      " >>> discussion about how to use Python to perform rule-based and phrase \n",
      " .words stop spaCy saw also we ,addition In .matching based <<< \n",
      " \n",
      ">>> In the , we will see parts of speech tagging and named entity \n",
      "                                               .detail in recognition <<< \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = Document('demo.docx')\n",
    "pars=document.paragraphs\n",
    "for par in pars:\n",
    "    candi=par.text\n",
    "    \n",
    "    try:\n",
    "        fname=par.runs[0].font.name\n",
    "        fsize=par.runs[0].font.size\n",
    "    except: pass\n",
    "#         print ('no runs')\n",
    "    par.clear()\n",
    "    flow=[]\n",
    "    width=70\n",
    "    lines=textwrap.wrap(candi, width)\n",
    "    for i in range (len(lines)):\n",
    "        line_index=i+1\n",
    "        if line_index%2:\n",
    "        \n",
    "#         flow.append(['>>>']+re.split('(\\W)',lines[i])+['\\r'])\n",
    "\n",
    "            flow = flow + ['>>>']+lines[i].split()+['\\r']\n",
    "        elif len(lines[i])< width/2:\n",
    "            flow = flow + [' ']*(int(width/3))  + reverse_sentense(lines[i].split())+['<<<']+['\\r']\n",
    "#         flow.append([' ']*(int(width/3))  + reverse_sentense(re.split('(\\W)',lines[i]))+['<<<']+['\\r'])\n",
    "\n",
    "        else:\n",
    "            flow = flow + reverse_sentense(lines[i].split())+['<<<']+['\\r']\n",
    "#         flow.append(reverse_sentense(re.split('(\\W)',lines[i]))+['<<<']+['\\r'])\n",
    "\n",
    "    if len(flow) > 0:\n",
    "        for word in flow:\n",
    "            nword=word + ' '\n",
    "            run = par.add_run(nword)\n",
    "            font = run.font\n",
    "            font.name = fname\n",
    "            font.size = fsize      \n",
    "            doc = nlp(word)\n",
    "            try:\n",
    "                for token in doc:\n",
    "\n",
    "                    if token.pos_=='NOUN':\n",
    "                        font.color.theme_color = MSO_THEME_COLOR.ACCENT_1\n",
    "            except:\n",
    "                print (\"???\", word)\n",
    "    print (par.text)\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperlink\n",
    "https://github.com/python-openxml/python-docx/issues/74\n",
    "https://stackoverflow.com/questions/47666642/adding-an-hyperlink-in-msword-by-using-python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "\n",
    "def add_hyperlink(paragraph, text, url):\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element and a new w:rPr element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Create a new Run object and add the hyperlink into it\n",
    "    r = paragraph.add_run ()\n",
    "    r._r.append (hyperlink)\n",
    "\n",
    "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
    "    # Delete this if using a template that has the hyperlink style in it\n",
    "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
    "    r.font.underline = True\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "document = docx.Document()\n",
    "p = document.add_paragraph('A plain paragraph having some ')\n",
    "add_hyperlink(p, 'Link to my site', \"http://supersitedelamortquitue.fr\")\n",
    "document.save('demo_hyperlink.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = docx.Document('demo_hyperlink.docx')\n",
    "pars=document.paragraphs\n",
    "# for par in pars:\n",
    "#     candi=par.text\n",
    "# #     print (candi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=pars[0]\n",
    "r=par.runs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument must be bytes or unicode, got 'builtin_function_or_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-935df357d0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._Element.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/apihelpers.pxi\u001b[0m in \u001b[0;36mlxml.etree._getAttributeValue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/apihelpers.pxi\u001b[0m in \u001b[0;36mlxml.etree._getNodeAttributeValue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/apihelpers.pxi\u001b[0m in \u001b[0;36mlxml.etree._getNsTag\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/apihelpers.pxi\u001b[0m in \u001b[0;36mlxml.etree.__getNsTag\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/apihelpers.pxi\u001b[0m in \u001b[0;36mlxml.etree._utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument must be bytes or unicode, got 'builtin_function_or_method'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document('demo.docx')\n",
    "pars=document.paragraphs\n",
    "for par in pars:\n",
    "    candi=par.text\n",
    "    \n",
    "    try:\n",
    "        fname=par.runs[0].font.name\n",
    "        fsize=par.runs[0].font.size\n",
    "    except: pass\n",
    "#         print ('no runs')\n",
    "    par.clear()\n",
    "    bow=re.split('(\\W)',candi)\n",
    "#     bow=candi.split()\n",
    "    if len(bow) > 0:\n",
    "        for word in bow:\n",
    "#             nword=word + ' '\n",
    "            nword=word\n",
    "\n",
    "            run = par.add_run(nword)\n",
    "            font = run.font\n",
    "            font.name = fname\n",
    "            font.size = fsize      \n",
    "            doc = nlp(word)\n",
    "            try:\n",
    "                for token in doc:\n",
    "\n",
    "                    if token.pos_=='NOUN':\n",
    "                        font.color.theme_color = MSO_THEME_COLOR.ACCENT_1\n",
    "            except:\n",
    "                print (\"???\", word)\n",
    "#     print (par.text)\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_pos(in_docx,out_docx):\n",
    "    document = Document(in_docx)\n",
    "    pars=document.paragraphs\n",
    "    for par in pars:\n",
    "        candi=par.text\n",
    "        par.clear()\n",
    "        \n",
    "        bow=candi.split()\n",
    "        if len(bow) > 0:\n",
    "            for word in bow:\n",
    "                nword=word + ' '\n",
    "                run = par.add_run(nword)\n",
    "                font = run.font\n",
    "#                 font.name = 'Calibri'\n",
    "#                 font.size = Pt(12)       \n",
    "                doc = nlp(word)\n",
    "                try:\n",
    "                    for token in doc:\n",
    "    \n",
    "                        if token.pos_=='NOUN':\n",
    "                            font.color.theme_color = MSO_THEME_COLOR.ACCENT_1\n",
    "                except:\n",
    "                    print (\"???\", word)\n",
    "    document.save(out_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_pos('demo.docx','demo.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
